# Python script for checking velocity and discharge convergence from the _PO.csv generated by TUFLOW
# Last updated on 12/04/2020 by SJP and KGL

# Step 1 - copy this script for use into your review folder
# Step 2 - set the ..\\results\\runID folder and the ..\\review\\runID folder
# Step 3 - check the files in the folder created (PO convergence times, criteria, and plots)

import os
import shutil
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import logging
FORMAT = ">>> %(filename)s, ln %(lineno)s - %(funcName)s: %(message)s"
logging.basicConfig(format=FORMAT, level=logging.INFO)

# SET THE FOLDERS HERE
# example: 'E:\\LYR\\LYR_2017studies\\LYR17_2Dmodelling\\LYR17_3_MRYFR\\results\\110'
results_folder = 'Z:\\LYR\\LYR_2017studies\\LYR17_2Dmodelling\\LYR17_1_EDDPD\\results\\133'
# example: 'E:\\LYR\\LYR_2017studies\\LYR17_2Dmodelling\\LYR17_3_MRYFR\\review'
review_folder = 'Z:\\LYR\\LYR_2017studies\\LYR17_2Dmodelling\\LYR17_1_EDDPD\\review'


def convergence_check(po_filename, check_slopes=False, *args, **kwargs):
    # number of values to check back for convergence
    check_length = 60
    # error tolerance (fractional difference) for velocity
    tolerance_V = 0.001
    # error tolerance (fractional difference) for discharge
    tolerance_Q = 0.001
    if check_slopes:
        # slope threshold for linear regression line [velocity/time]
        slope_thr_V = 1
        # slope threshold for linear regression line [discharge/time]
        slope_thr_Q = 1
    else:
        slope_thr_V = np.nan
        slope_thr_Q = np.nan

    # flatten array function (used to calculate max convergence times below)
    flatten = lambda l: [val[0] for val in l]

    # get header PO names
    with open(po_filename) as f:
        lines = f.readlines()
        if lines:
            header = lines[1]
            header = header.replace("\"", "").replace("\n", "").split(',')[1:]
        else:
            logging.info(f'WARNING: Skipping blank PO file: {po_filename}')
            return

    # parse PO csv to dataframe
    df = pd.read_csv(po_filename, header=None, skiprows=3)
    df = df.drop(0, axis=1)
    df.columns = header
    pos = header[1:]

    # iterate over PO and plot/save to .png
    for po in pos:
        plt.clf()
        x = df.Time
        plt.xlabel('Time (hrs)', fontsize=12)
        y = df[po]
        plt.plot(x, y)
        plt.title(po)

        if po.startswith('Pt'):
            plt.ylabel('Velocity (ft/s)', fontsize=12)
        else:
            plt.ylabel('Discharge (cfs)', fontsize=12)

        plt.savefig(po_filename.replace('PO.csv', po + '.png'))

    # check that PO name convention begins with 'Pt' or 'Ln'
    for po in pos:
        if not (po.startswith('Pt') or po.startswith('Ln')):
            raise SyntaxError(
                "Name convention not satisfied for: %s.\nVelocity point observation names should begin with \'Pt\'\nDischarge line observations should begin with \'Ln\'" % po)

    # initialize convergence times dict ('DNC' = does not converge)
    convergence_time = {po: ['DNC'] for po in pos}

    # iterate over po locations
    for po in pos:
        logging.info('checking %s...' % po)
        if po.startswith('Pt'):
            slope_thr = slope_thr_V
            tolerance = tolerance_V
        if po.startswith('Ln'):
            slope_thr = slope_thr_Q
            tolerance = tolerance_Q
        # iterate down column
        for i in range(check_length, len(df.Time)):
            # collect last 20 values and check slope of linear regression line
            t = df.Time[i - check_length: i]
            vals = df[po][i - check_length: i]
            if check_slopes:
                slope, yint = np.polyfit(t, vals, deg=1)
            else:
                slope = 0
            # calculate fractional differences between value at i previous values
            frac_diffs = []
            # if current check value is not 0
            if df[po][i]:
                # iterate over previous check_length values
                for di in range(check_length):
                    # if previous value in also not 0
                    if df[po][i - check_length + di]:
                        # calculate fractional difference between previous values and check value
                        frac_diff = abs((df[po][i] - df[po][i - check_length + di]) / df[po][i])
                        frac_diffs.append(frac_diff)
                    # skip to next timestep if any df[po][i - check_length + di] == 0
                    else:
                        frac_diffs.append(np.nan)
                        break
            # skip to next timestep if df[po][i] == 0
            else:
                frac_diffs.append(np.nan)

            if not np.nan in frac_diffs:
                # check if previous check_val (e.g. 20) values are all within tolerance
                if np.all(np.asarray(frac_diffs) <= tolerance):
                    # check if slope of linear regression is within slope threshold
                    if not abs(slope) > slope_thr:
                        convergence_time[po] = [df.Time[i]]
                        break
            # if break didn't occur on last iteration, save amount still fluctuating
            if i == len(df.Time) - 1:
                if not np.nan in frac_diffs:
                    convergence_time[po] = [f'DNC: {max(frac_diffs) * 100:.2f}% max diff']

    # make separate dataframes for monitoring points (for velocity) and lines (for discharge)
    pts = {i: v for i, v in convergence_time.items() if i.startswith('Pt')}
    pts_df = pd.DataFrame.from_dict(pts)
    lns = {i: v for i, v in convergence_time.items() if i.startswith('Ln')}
    lns_df = pd.DataFrame.from_dict(lns)

    # aggregate comments for DNC points
    comments = 'max diffs for DNC POs: '
    for po, val in convergence_time.items():
        if 'DNC' in str(val[0]):
            comments += f'{po}{val[0].replace("DNC", "").replace(" max diff", "")}, '

    # save dataframe of convergence time for all observation points/lines
    conv_df = pd.DataFrame.from_dict(convergence_time)

    try:
        conv_df['V conv time'] = [max(flatten(pts.values()))]
    except:
        conv_df['V conv time'] = ['DNC']
    try:
        conv_df['Q conv time'] = [max(flatten(lns.values()))]
    except:
        conv_df['Q conv time'] = ['DNC']

    conv_df['Comments'] = [comments]
    conv_path = po_filename.replace('.csv', '_convergence_times.csv')
    conv_df.to_csv(conv_path, index=False)
    logging.info('\nsaved convergence time table: %s\n' % conv_path)

    # print summary to console
    logging.info(pts_df)
    logging.info(lns_df)

    try:
        logging.info('\nvelocity convergence at t = %.2f' % max(flatten(pts.values())))
    except TypeError:
        logging.info('\nvelocities DO NOT CONVERGE!')

    try:
        logging.info('discharge convergence at t = %.4f' % max(flatten(lns.values())))
    except TypeError:
        logging.info('discharges DO NOT CONVERGE!')

    if check_slopes:
        logging.info(
                    '\nvelocity convergence = previous %i values have less than %.4f%% deviation and met the slope threshold value of %.4f' % (
            check_length, tolerance_V * 100, slope_thr_V))
        logging.info(
                    '\ndischarge convergence = previous %i values have less than %.4f%% deviation and met the slope threshold value of %.4f' % (
            check_length, tolerance_Q * 100, slope_thr_Q))
    else:
        logging.info(
            '\nvelocity convergence = previous %i values have less than %.4f%% deviation.' % (
                check_length, tolerance_V * 100))
        logging.info(
            '\ndischarge convergence = previous %i values have less than %.4f%% deviation.' % (
                check_length, tolerance_Q * 100))

    # create text file recording the error tolerance and slope thresholds set
    text_path = conv_path.replace('.csv', '_convergence_criteria.txt')
    f = open(text_path, 'w+')
    f.write('\ncheck length = previous %i values were checked' % (check_length))
    f.write('\nerror tolerance for velocity = %.4f%%' % (tolerance_V * 100))
    if check_slopes:
        f.write('\nslope threshold for velocity = %.4f' % (slope_thr_V))
    f.write('\nerror tolerance for discharge = %.4f%%' % (tolerance_Q * 100))
    if check_slopes:
        f.write('\nslope threshold for discharge = %.4f' % (slope_thr_Q))
    f.close()


if __name__ == "__main__":
    for dir, subdirs, files in os.walk(results_folder):
        for PO_file in files:
            if PO_file.endswith('_PO.csv'):

                run_id = PO_file.split('_')[2]
                run_id_folder = os.path.join(review_folder, run_id)
                discharge = PO_file.replace('_PO.csv', '')
                discharge_folder = os.path.join(run_id_folder, discharge)

                # create new folder for run ID and subfolder for discharge, if doesn't exist
                if not os.path.exists(discharge_folder):
                    logging.info('Creating discharge folder for {0}'.format(discharge))
                    os.makedirs(discharge_folder)

                source_path = os.path.join(dir, PO_file)  # path of PO file in results folder
                po_filename = os.path.join(discharge_folder, PO_file)  # path of PO file in review/runID/discharge folder

                # copy PO file to review/runID/discharge folder
                shutil.copy(source_path, discharge_folder)
                logging.info('PO.csv files have been copied from the results folder to discharge folder for {0}'.format(discharge))
                # run convergence check
                logging.info('Running convergence check function for {0}'.format(discharge_folder))
                convergence_check(po_filename)
